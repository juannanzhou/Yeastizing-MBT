{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adfe893b-ecdb-47d7-be60-6495b1d43ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# get list of all protein IDs\n",
    "protein_list = []\n",
    "for file in glob.glob('Data/*.csv'):\n",
    "    protein_list.append(file[5:9])\n",
    "\n",
    "# generate a list of all dataframes\n",
    "dfs_list = []\n",
    "\n",
    "for protein in protein_list:\n",
    "  df = pd.read_csv(\"Data/\" + protein + \"_seq_lab.csv\")\n",
    "  df[\"protein_id\"] = protein\n",
    "  dfs_list.append(df)\n",
    "\n",
    "# merge all dataframes to one dataframe\n",
    "data_all = pd.concat(dfs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2f030b5-1266-438c-899e-a8f8fccb5bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "AAs = \"CAGPSTNDEQRHKILMVFWY\"\n",
    "groups = [AA for AA in AAs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "becd78c5-f335-4cdd-9c71-dcae38ed4fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def count_mer(mer, seq):\n",
    "    '''\n",
    "    Counts the number of times a substring mer\n",
    "    ocurrs in the sequence seq (including overlapping\n",
    "    occurrences)\n",
    "\n",
    "    sample use: count_mer(\"GGG\", \"AGGGCGGG\") => 2\n",
    "    '''\n",
    "\n",
    "    k = len(mer)\n",
    "    count = 0\n",
    "    for i in range(0, len(seq)-k+1):\n",
    "        if mer == seq[i:i+k]:\n",
    "            count = count + 1\n",
    "    return count\n",
    "    \n",
    "def kmer_count(mers, seq):\n",
    "    '''\n",
    "    Return a list of the number of times each possible k-mer appears\n",
    "    in seq, including overlapping occurrences.\n",
    "    '''\n",
    "\n",
    "    rv = []\n",
    "    for m in mers:\n",
    "        cnt = count_mer(m, seq)        \n",
    "        rv.append(cnt)\n",
    "    return rv\n",
    "\n",
    "def get_counts(mers, seqs):\n",
    "    counts = [kmer_count(mers, seq) for seq in seqs]\n",
    "    return np.array(counts)\n",
    "\n",
    "def recode_seq(text, args_dict):\n",
    "    for key in args_dict.keys():\n",
    "        text = text.replace(key, str(args_dict[key]))\n",
    "    return text\n",
    "\n",
    "class Preprocessing:\n",
    "\n",
    "    def __init__(self, data, args_dict, num_words, seq_len=1024):\n",
    "        self.data = data\n",
    "        self.args_dict = args_dict\n",
    "        self.seq_len = seq_len\n",
    "        self.num_words = num_words        \n",
    "        self.vocabulary = None\n",
    "        self.x_tokenized = None\n",
    "        self.x_padded = None\n",
    "        self.x_raw = None\n",
    "        \n",
    "\n",
    "        self.x_train = None\n",
    "        self.x_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        \n",
    "        self.X_counts_ = {}\n",
    "\n",
    "    def load_data(self):\n",
    "        # Reads the raw csv file and split into\n",
    "        # sentences (x) and target (y)\n",
    "\n",
    "        df = self.data\n",
    "\n",
    "        self.x_raw = df['seqs'].values\n",
    "        self.y = torch.tensor(df['labels'].values).cuda()        \n",
    "\n",
    "    def text_tokenization(self):\n",
    "        # Tokenizes each sentence by implementing the nltk tool\n",
    "        self.x_converted = [recode_seq(x, self.args_dict) for x in self.x_raw]\n",
    "\n",
    "    def build_vocabulary(self):\n",
    "        # Builds the vocabulary and keeps the \"x\" most frequent words\n",
    "        self.vocabulary = dict()\n",
    "        fdist = nltk.FreqDist()\n",
    "\n",
    "        for sentence in self.x_converted:\n",
    "            for word in sentence:\n",
    "                fdist[word] += 1\n",
    "\n",
    "        common_words = fdist.most_common(self.num_words)\n",
    "\n",
    "        for idx, word in enumerate(common_words):\n",
    "            self.vocabulary[word[0]] = (idx+1)\n",
    "  \n",
    "    def word_to_idx(self):\n",
    "        # By using the dictionary (vocabulary), it is transformed\n",
    "        # each token into its index based representation\n",
    "\n",
    "        self.x_tokenized = list()\n",
    "\n",
    "        for sentence in self.x_converted:\n",
    "            temp_sentence = list()\n",
    "            for word in sentence:\n",
    "                if word in self.vocabulary.keys():\n",
    "                    temp_sentence.append(self.vocabulary[word])\n",
    "            self.x_tokenized.append(temp_sentence)\n",
    "\t      \n",
    "    def padding_sentences(self):\n",
    "        # Each sentence which does not fulfill the required len\n",
    "        # it's padded with the index 0\n",
    "\n",
    "        pad_idx = 0\n",
    "        self.x_padded = []\n",
    "        \n",
    "        for sentence in self.x_tokenized:\n",
    "            if len(sentence) > self.seq_len:\n",
    "                sentence = sentence[:self.seq_len]\n",
    "            else: pass\n",
    "            while len(sentence) < self.seq_len:\n",
    "                sentence.insert(len(sentence), pad_idx)\n",
    "            self.x_padded.append(sentence)\n",
    "        self.X = torch.tensor(np.array(self.x_padded)).cuda()\n",
    "        \n",
    "    def mer_counts(self, k):\n",
    "        alphabet_reduced = list(range(len(groups)))\n",
    "        alphabet_reduced = [str(a) for a in alphabet_reduced]\n",
    "\n",
    "        k_mers = list(product(*[alphabet_reduced for i in range(k)]))\n",
    "        k_mers = [(\"\").join(list(k_mer)) for k_mer in k_mers]\n",
    "        \n",
    "        mer_counts = get_counts(k_mers, self.x_converted)\n",
    "        self.X_counts_[k] = torch.tensor(mer_counts).cuda()\n",
    "        \n",
    "        \n",
    "    def prep_data(self, train_prots, counts):\n",
    "\n",
    "        train_bool = [ID in set(train_prots) for ID in self.data.protein_id]\n",
    "        test_bool = [ID not in set(train_prots) for ID in self.data.protein_id]\n",
    "        \n",
    "        if counts:\n",
    "            X = self.X_counts\n",
    "            # X = X/X.sum(1).unsqueeze(1)\n",
    "        else: X = self.X\n",
    "        train_x, train_y = X[train_bool], self.y[train_bool]\n",
    "        \n",
    "        test_x, test_y = X[test_bool], self.y[test_bool]    \n",
    "\n",
    "        data = {}\n",
    "        data[\"x_train\"] = train_x\n",
    "        data[\"y_train\"] = train_y.float()\n",
    "        data[\"x_test\"] = test_x\n",
    "        # data[\"y_test\"] = test_y[np.random.permutation(len(test_y))]\n",
    "        data[\"y_test\"] = test_y.float()\n",
    "\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b16cca1-937f-43d9-940d-591add373855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "k = 2\n",
    "\n",
    "alphabet_reduced = list(range(len(groups)))\n",
    "alphabet_reduced = [str(a) for a in alphabet_reduced]\n",
    "\n",
    "k_mers = list(product(*[alphabet_reduced for i in range(k)]))\n",
    "k_mers = [(\"\").join(list(k_mer)) for k_mer in k_mers]\n",
    "\n",
    "AA2G = {}\n",
    "for i, group in enumerate(groups):\n",
    "    for letter in group:\n",
    "        AA2G[letter] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88870a86-a630-4065-a46c-b426e220d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = Preprocessing(data_all, AA2G, num_words=25, seq_len=1024)\n",
    "pre.load_data()\n",
    "pre.text_tokenization()\n",
    "pre.build_vocabulary()\n",
    "pre.word_to_idx()\n",
    "pre.padding_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2b41ad9-dedf-4f77-894b-9234d05853f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre.mer_counts(1)\n",
    "pre.mer_counts(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7503d7-a71b-4ce9-8fc8-d2517c166127",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24436af8-5050-4d4e-8fd2-1b96f0f89a48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8018909-2a05-4a01-97d5-f87e671b0d51",
   "metadata": {},
   "source": [
    "#### Dimers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e46154e0-0ce3-4a34-97ac-3beaae9de92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre.X_counts = pre.X_counts_[2]\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "auroc_list = []\n",
    "coef_list = []\n",
    "complexity_param = 10\n",
    "\n",
    "\n",
    "for i in range(len(protein_list)):\n",
    "    train_prots = pd.Series(protein_list).drop(i).tolist()\n",
    "    data = pre.prep_data(train_prots, True)\n",
    "    model = LogisticRegression(random_state=0, penalty='l1', solver=\"liblinear\", C=complexity_param)\n",
    "    clf = model.fit(data['x_train'].cpu().numpy(), data['y_train'].cpu().numpy())\n",
    "    predict_y = clf.predict(data['x_test'].cpu().numpy())\n",
    "    predict_p = clf.predict_proba(data['x_test'].cpu().numpy())    \n",
    "    auroc = roc_auc_score(data['y_test'].cpu().numpy(), predict_y)\n",
    "    auroc_list.append(auroc)\n",
    "    coef_list.append(clf.coef_)\n",
    "\n",
    "results[\"Dimer\"] = auroc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b6633e-357b-42ec-bf77-170b236f051b",
   "metadata": {},
   "source": [
    "#### Monomers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0838ba64-1603-466f-a39c-4e8432238c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre.X_counts = pre.X_counts_[1]\n",
    "\n",
    "auroc_list = []\n",
    "complexity_param = 1\n",
    "for i in range(len(protein_list)):\n",
    "    train_prots = pd.Series(protein_list).drop(i).tolist()\n",
    "    data = pre.prep_data(train_prots, True)\n",
    "    model = LogisticRegression(random_state=0, penalty='l2', solver=\"liblinear\", C=complexity_param)\n",
    "    clf = model.fit(data['x_train'].cpu().numpy(), data['y_train'].cpu().numpy())\n",
    "    predict_y = clf.predict(data['x_test'].cpu().numpy())\n",
    "    predict_p = clf.predict_proba(data['x_test'].cpu().numpy())    \n",
    "    auroc = roc_auc_score(data['y_test'].cpu().numpy(), predict_y)\n",
    "    auroc_list.append(auroc)\n",
    "\n",
    "results[\"Monomer\"] = auroc_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62a07022-4d82-4a7a-8b27-921d2c8917c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'AUROC_list_CNN.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/scratch/local/37499389/ipykernel_2471690/1110435620.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CNN\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AUROC_list_CNN.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Monomer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Dimer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CNN\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplotdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Monomer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dimer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CNN\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwidths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/nlp/1.2/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/nlp/1.2/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/nlp/1.2/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/nlp/1.2/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/nlp/1.2/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/nlp/1.2/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/nlp/1.2/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    648\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'AUROC_list_CNN.csv'"
     ]
    }
   ],
   "source": [
    "results[\"CNN\"] = pd.read_csv(\"AUROC_list_CNN.csv\").to_numpy().flatten()\n",
    "x_labels = [\"Monomer\", \"Dimer\", \"CNN\"]\n",
    "\n",
    "plotdata = [results[\"Monomer\"], results[\"Dimer\"], results[\"CNN\"]]\n",
    "widths = [0.5, 0.5, 0.5]  \n",
    "\n",
    "\n",
    "# Create the boxplot with narrower boxes\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Adjust the width of the boxes (e.g., set width to 0.5 for narrower boxes)\n",
    "boxplot = ax.boxplot(plotdata, patch_artist=True, widths=0.2)\n",
    "\n",
    "# Set the x-labels and y-axis label as needed\n",
    "ax.set_xticklabels(x_labels, fontsize=20)\n",
    "ax.set_ylabel('Test AUROC', fontsize=20)\n",
    "\n",
    "# Customize the box colors and edge colors (optional)\n",
    "for box in boxplot['boxes']:\n",
    "    box.set(facecolor='gray', edgecolor='black')\n",
    "\n",
    "median_color = 'orange'\n",
    "for median in boxplot['medians']:\n",
    "    median.set(color=median_color, linewidth=3.5)  # Adjust linewidth as needed\n",
    "    \n",
    "ax.tick_params(axis='y', labelsize=20)\n",
    "ax.grid(True, linestyle='--', alpha=0.8)  # You can customize linestyle and alpha as needed\n",
    "\n",
    "# Save and show the plot\n",
    "plt.savefig('figs/auroc_box.pdf', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03544fc6-6add-4614-8514-8c828aea186d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualization of counts data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4df018-58e5-4bbc-93ca-319ed3c5cb6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Dimer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77761e23-532c-4018-a477-8a16b433ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all[data_all.protein_id == \"5296\"].to_csv(\"seq_data.csv\")\n",
    "\n",
    "counts_raw = pre.X_counts_[2]\n",
    "\n",
    "freqs = counts_raw/counts_raw.sum(1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6014f766-945c-4838-961d-9c5d77e9aa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "X = freqs.detach().cpu().numpy()\n",
    "X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
    "                  init='random', perplexity=3).fit_transform(X)\n",
    "X_embedded.shape\n",
    "\n",
    "\n",
    "plotdf = pd.DataFrame({\"x1\":X_embedded[:,0], \"x2\": X_embedded[:, 1], \"lab\": data_all.labels})\n",
    "\n",
    "plotdf = plotdf.rename(columns={\"x1\":\"tSNE1\", \"x2\": \"tSNE2\", \"lab\":\"Species\"})\n",
    "\n",
    "plotdf.Species = plotdf.Species.replace({0: \"Fungal\", 1: \"Plant\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "daf1023f-8e02-4ee0-9db2-6184a00ee75c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plotdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/scratch/local/37499389/ipykernel_2471690/2770371341.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Your data plotting code here (e.g., sns.relplot)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplotdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tSNE1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tSNE2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Species'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m# Get the current axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plotdf' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "sns.set(font_scale=1.2)  \n",
    "sns.set_style(\"ticks\") \n",
    "\n",
    "plt.figure(figsize=(8, 8)) \n",
    "\n",
    "# Your data plotting code here (e.g., sns.relplot)\n",
    "\n",
    "sns.relplot(data=plotdf, x='tSNE1', y='tSNE2', hue='Species', aspect=1, alpha=0.5)\n",
    "# Get the current axes\n",
    "ax = plt.gca()\n",
    "\n",
    "# # Add ticks to the x-axis and y-axis\n",
    "plt.xticks(range(-100, 120, 50))  # Specify the tick positions for the x-axis\n",
    "plt.yticks(range(-100, 120, 50))  # Specify the tick positions for the x-axis\n",
    "\n",
    "\n",
    "plt.savefig('figs/dimer_tSNE.pdf', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f0f69c-34be-408a-91f6-a866405f4e41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-1.2",
   "language": "python",
   "name": "nlp-1.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
